{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir) \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dense, Input, Bidirectional, LSTM, Dropout, CuDNNLSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import pickle\n",
    "import codecs\n",
    "import csv\n",
    "import struct\n",
    "\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, LSTM, Dropout, CuDNNLSTM\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file path\n",
    "Guryongpo_Swell_Data_Path = './Data/1.Guryongpo_Swell(14~17).csv'\n",
    "Wolopo_Swell_Data_Path = './Data/2.Wolpo_Swell(15~17).csv'\n",
    "Pohang_Weather_Data_Path = './Data/3.Pohang_Weather(14~17).csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data list\n",
    "Guryongpo_Swell = []\n",
    "Wolopo_Swell = []\n",
    "Guryongpo_Swell_with_Weather = []\n",
    "Wolopo_Swell_with_Weather = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string list \n",
    "\n",
    "# Only Weather\n",
    "w = ['평균기온(°C)', '최저기온(°C)','최저기온 시각(hhmi)','최고기온(°C)',\n",
    "      '최고기온 시각(hhmi)','강수 계속시간(hr)','일강수량(mm)','최대 순간 풍속(m/s)',\n",
    "      '최대 순간 풍속 풍향(16방위)','최대 순간풍속 시각(hhmi)','최대 풍속(m/s)',\n",
    "      '최대 풍속 풍향(16방위)','최대 풍속 시각(hhmi)','평균 풍속(m/s)','풍정합(100m)',\n",
    "      '평균 이슬점온도(°C)','최소 상대습도(%)','최소 상대습도 시각(hhmi)',\n",
    "      '평균 상대습도(%)','평균 증기압(hPa)','평균 현지기압(hPa)','최고 해면기압(hPa)',\n",
    "      '최고 해면기압 시각(hhmi)','최저 해면기압(hPa)','최저 해면기압 시각(hhmi)',\n",
    "      '평균 해면기압(hPa)','가조시간(hr)','합계 일조 시간(hr)','1시간 최다일사 시각(hhmi)',\n",
    "      '1시간 최다일사량(MJ/m2)','합계 일사(MJ/m2)','평균 전운량(1/10)','평균 중하층운량(1/10)',\n",
    "      '평균 지면온도(°C)','최저 초상온도(°C)','평균 5cm 지중온도(°C)','평균 10cm 지중온도(°C)',\n",
    "      '평균 20cm 지중온도(°C)','평균 30cm 지중온도(°C)','0.5m 지중온도(°C)','1.0m 지중온도(°C)',\n",
    "      '1.5m 지중온도(°C)','3.0m 지중온도(°C)','5.0m 지중온도(°C)','합계 대형증발량(mm)',\n",
    "      '합계 소형증발량(mm)']\n",
    "    \n",
    "# Only Swell\n",
    "o = ['평균 수온(°C)', '최고 수온(°C)', '최저 수온(°C)', '평균 유의 파고(m)',\n",
    "     '평균 파고(m)', '최고 유의 파고(m)', '최고 최대 파고(m)','평균 파주기(sec)', '최고 파주기(sec)',]\n",
    "\n",
    "\n",
    "# Guryongpo_Swell + Wolpo_Swell\n",
    "m = ['G_평균 수온(°C)','W_평균 수온(°C)', 'G_최고 수온(°C)','W_최고 수온(°C)', \n",
    "     'G_최저 수온(°C)', 'W_최저 수온(°C)', 'G_평균 유의 파고(m)','W_평균 유의 파고(m)','G_평균 파고(m)',\n",
    "     'W_평균 파고(m)', 'G_최고 유의 파고(m)','W_최고 유의 파고(m)', 'G_최고 최대 파고(m)','W_최고 최대 파고(m)',\n",
    "     'G_평균 파주기(sec)','W_평균 파주기(sec)', 'G_최고 파주기(sec)', 'W_최고 파주기(sec)']\n",
    "\n",
    "# Swell + Weather\n",
    "c = np.concatenate([o,w], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "print(len(c))\n",
    "print(len(m)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data after merge\n",
    "Column_Sum_Swell = []\n",
    "Row_Sum_Swell = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data list\n",
    "Pohang_Weather = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe \n",
    "Guryongpo_DF = []\n",
    "Guryongpo_Swell_with_Weather_DF =[]\n",
    "\n",
    "Wolopo_DF = []\n",
    "Wolopo_Swell_with_Weather_DF = []\n",
    "\n",
    "Pohang_DF = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe \n",
    "Column_Sum_DF = []\n",
    "Row_Sum_DF = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file \n",
    "def loadData():   \n",
    "    print(\"Data loading...\")\n",
    "    \n",
    "    global Guryongpo_Swell\n",
    "    global Wolopo_Swell\n",
    "    global Pohang_Weather\n",
    "    \n",
    "    global Guryongpo_Swell_with_Weather\n",
    "    global Wolopo_Swell_with_Weather\n",
    "    \n",
    "    \n",
    "    global Guryongpo_DF\n",
    "    global Guryongpo_Swell_with_Weather_DF\n",
    "    \n",
    "    global Wolopo_DF\n",
    "    global Wolopo_Swell_with_Weather_DF\n",
    "    \n",
    "    global Pohang_DF\n",
    "    \n",
    "    # Read csv file from directory\n",
    "    Guryongpo = genfromtxt(Guryongpo_Swell_Data_Path, delimiter=',' ,encoding='UTF8')\n",
    "    Wolopo = genfromtxt(Wolopo_Swell_Data_Path, delimiter=',', encoding='UTF8')\n",
    "    Pohang = genfromtxt(Pohang_Weather_Data_Path, delimiter=',', encoding='UTF8')\n",
    "    \n",
    "    \n",
    "    # Remove colums and row\n",
    "    Guryongpo_Swell = Guryongpo[1:,3:]\n",
    "    Wolopo_Swell = Wolopo[1:,3:]\n",
    "    Pohang_Weather = Pohang[1:,2:]\n",
    "    \n",
    "    \n",
    "    # Convert string to float64\n",
    "    Guryongpo_Swell.astype('float64')\n",
    "    Wolopo_Swell.astype('float64')\n",
    "    Pohang_Weather.astype('float64')\n",
    " \n",
    "   \n",
    "    # Fill NaN data to 0   \n",
    "    Guryongpo_Swell[np.isnan(Guryongpo_Swell)] = 0\n",
    "    Wolopo_Swell[np.isnan(Wolopo_Swell)] = 0\n",
    "    Pohang_Weather[np.isnan(Pohang_Weather)] = 0\n",
    "\n",
    "    \n",
    "    # Guryongpo  - Wolopo\n",
    "    dis = len(Guryongpo_Swell)- len(Wolopo_Swell)\n",
    "    \n",
    "    \n",
    "    # Concatenate between swell and weather data\n",
    "    Guryongpo_Swell_with_Weather = np.concatenate([Guryongpo_Swell,Pohang_Weather], axis=1)\n",
    "    Wolopo_Swell_with_Weather = np.concatenate([Wolopo_Swell,Pohang_Weather[dis:]], axis=1)\n",
    "    \n",
    "     \n",
    "    # Dataframe\n",
    "    Guryongpo_DF = pd.DataFrame(Guryongpo_Swell, columns = o)\n",
    "    Guryongpo_Swell_with_Weather_DF = pd.DataFrame(Guryongpo_Swell_with_Weather, columns = c)\n",
    "    \n",
    "    Wolopo_DF = pd.DataFrame(Wolopo_Swell, columns = o)\n",
    "    Wolopo_Swell_with_Weather_DF = pd.DataFrame(Wolopo_Swell_with_Weather, columns = c)\n",
    "\n",
    "    Pohang_DF = pd.DataFrame(Pohang_Weather, columns = w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading...\n"
     ]
    }
   ],
   "source": [
    "loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1438, 46)\n"
     ]
    }
   ],
   "source": [
    "print(Pohang_Weather.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015-09-25 부터 colum sum \n",
    "# 구룡포의 2014 데이터는 어떻게 할 것인지 보류 (2014 구룡포 데이터는 있지만 월포의 데이터가 없음)\n",
    "# Data merge by column\n",
    "def sumColumnData():\n",
    "    print(\"process colum sum...\")\n",
    "    global Guryongpo_Swell\n",
    "    global Wolopo_Swell\n",
    "    global Column_Sum_Swell\n",
    "    global Column_Sum_DF\n",
    "    \n",
    "    global c\n",
    "    global m\n",
    "    \n",
    "    for i in range(len(Guryongpo_Swell)):\n",
    "        dis = len(Guryongpo_Swell)- len(Wolopo_Swell)\n",
    "        if(i< dis):\n",
    "            continue\n",
    "        else:\n",
    "            data = []\n",
    "            for colum in range(len(Guryongpo_Swell[0])):\n",
    "                g = Guryongpo_Swell[i][colum]\n",
    "                w = Wolopo_Swell[i-dis][colum]\n",
    "                data.append(g)\n",
    "                data.append(w)\n",
    "            \n",
    "            for w in range(len(Guryongpo_Swell[0]),len(Guryongpo_Swell_with_Weather[0])):    \n",
    "                weather = Guryongpo_Swell_with_Weather[i][w]\n",
    "                data.append(weather)\n",
    "            Column_Sum_Swell.append(data)    \n",
    "       \n",
    "    t = np.concatenate([m,c[9:]])\n",
    "    Column_Sum_Swell = np.array(Column_Sum_Swell)\n",
    "    Column_Sum_DF = pd.DataFrame(Column_Sum_Swell, columns = t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data merge by row\n",
    "def sumRowData():\n",
    "    global Guryongpo_Swell\n",
    "    global Wolopo_Swell\n",
    "    global Row_Sum_Swell\n",
    "    global Row_Sum_DF\n",
    "    global c\n",
    "    \n",
    "    for i in range(len(Guryongpo_Swell_with_Weather)):\n",
    "        dis = len(Guryongpo_Swell_with_Weather)- len(Wolopo_Swell_with_Weather)\n",
    "        if(i< dis):\n",
    "            g = Guryongpo_Swell_with_Weather[i]\n",
    "            Row_Sum_Swell.append(g)\n",
    "        else:\n",
    "            g = Guryongpo_Swell_with_Weather[i]\n",
    "            w = Wolopo_Swell_with_Weather[i-dis]\n",
    "            Row_Sum_Swell.append(g)\n",
    "            Row_Sum_Swell.append(w)  \n",
    "            \n",
    "    Row_Sum_Swell = np.array(Row_Sum_Swell)\n",
    "    Row_Sum_DF = pd.DataFrame(Row_Sum_Swell, columns = c)\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전 :  (1438, 1)\n",
      "전 :  (1438, 46)\n",
      "후 :  (1377, 1)\n",
      "후 :  (1377, 46)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "global Guryongpo_Swell\n",
    "global Pohang_Weather\n",
    "global o # Swell data column string\n",
    "global w # Weather data column string\n",
    "'''\n",
    "    \n",
    "# data split by each column\n",
    "target = []\n",
    "weather = []\n",
    "\n",
    "# 이거 index값을 기억해뒀다가 나중에 predict한값들로 데이터 채우기\n",
    "delete_index = [] # nparray에서 delete할 index들의 list \n",
    "\n",
    "# 비어있는 값들  (x값은 있음, y값 비어있음)\n",
    "empty_x = [] # y값 비어있는 곳 predict에 사용할 weather정보\n",
    "empty_y = [] # weather 정보를 이용해 예측할 swell information의 컬럼 값집함\n",
    "\n",
    "weather = np.array(np.copy(Pohang_Weather))\n",
    "target = np.copy(Guryongpo_Swell[:,0])\n",
    "target = np.reshape(target, (target.shape[0],1))\n",
    "\n",
    "\n",
    "print(\"전 : \" ,target.shape)\n",
    "print(\"전 : \" ,weather.shape)\n",
    "for i in range(len(target)):\n",
    "    if(target[i]==0):\n",
    "        delete_index.append(i)\n",
    "        empty_y.append(target[i])\n",
    "        empty_x.append(weather[i])\n",
    "\n",
    "        \n",
    "# 값이 비어있는 row는 분리 - > 나중에 predict해서 채울수 있도록\n",
    "# 분리되지 않은 데이터들을 가지고 train / validation / test 로 나눠서 LSTM 모델을 만든다\n",
    "# 만든 모델을 가지고 비어있던 값들을 empty_x -> empty _y로 예측 해서 값을 채워넣는다.\n",
    "target = np.delete(target, delete_index)\n",
    "target = np.reshape(target, (target.shape[0],1))\n",
    "weather = np.delete(weather, delete_index, 0)\n",
    "\n",
    "empty_x = np.array(empty_x)\n",
    "empty_y = np.array(empty_y)\n",
    "#return columnData\n",
    "\n",
    "print(\"후 : \",target.shape)\n",
    "print(\"후 : \",weather.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where all models are saved\n",
    "BASE_PATH = './swellModel/'\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    os.mkdir(BASE_PATH)\n",
    "\n",
    "def create_checkpoint(model_name):\n",
    "    # creates a subdirectory under `BASE_PATH`\n",
    "    MODEL_PATH = os.path.join(BASE_PATH, model_name)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)\n",
    "    \n",
    "    return ModelCheckpoint(filepath=os.path.join(MODEL_PATH, '{epoch:02d}-{val_loss:.4f}.hdf5'),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_x = np.arange(len(target))\n",
    "plot_y = target\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1377, 46)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 데이터셋 생성하기\n",
    "signal_data = np.cos(np.arange(1600)*(20*np.pi/1000))[:,None]\n",
    "\n",
    "# 데이터 전처리\n",
    "signal_data.shape\n",
    "weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_dataset(signal_data, look_back):\n",
    "feature_num = weather.shape[1]\n",
    "\n",
    "'''\n",
    "def create_dataset(swell_data, weather_data, look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(swell_data)-look_back):\n",
    "        dataX.append(weather_data[i:(i+look_back), :])\n",
    "        dataY.append(swell_data[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "'''\n",
    "\n",
    "def create_dataset(swell_data, look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(swell_data)-look_back):\n",
    "        dataX.append(swell_data[i:(i+look_back), :])\n",
    "        dataY.append(swell_data[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "class CustomHistory(keras.callbacks.Callback):\n",
    "    def init(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "\n",
    "\n",
    "look_back = 14\n",
    "\n",
    "# 데이터 분리\n",
    "train_swell = target[0:int(len(target)*0.5)]\n",
    "val_swell = target[int(len(target)*0.5):int(len(target)*0.75)]\n",
    "test_swell = target[int(len(target)*0.75):]\n",
    "\n",
    "train_weather = weather[0:int(len(weather)*0.5)]\n",
    "val_weather = weather[int(len(weather)*0.5):int(len(weather)*0.75)]\n",
    "test_weater = weather[int(len(weather)*0.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n",
      "1377\n",
      "----------------------------\n",
      "1377\n",
      "1377\n"
     ]
    }
   ],
   "source": [
    "print(len(train_swell)+len(val_swell)+len(test_swell))\n",
    "print(len(target))\n",
    "print(\"----------------------------\")\n",
    "print(len(train_weather)+len(val_weather)+len(test_weater))\n",
    "print(len(weather))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.7],\n",
       "       [14. ],\n",
       "       [14.4],\n",
       "       [14.5],\n",
       "       [14.4],\n",
       "       [14.1],\n",
       "       [13.9],\n",
       "       [13.8],\n",
       "       [13.8],\n",
       "       [13.5]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_swell[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "'''\n",
    "x_train, y_train = create_dataset(train_swell,train_weather, look_back)\n",
    "x_val, y_val = create_dataset(val_swell,val_weather, look_back)\n",
    "x_test, y_test = create_dataset(test_swell,test_weater, look_back)\n",
    "'''\n",
    "x_train, y_train = create_dataset(train_swell, look_back)\n",
    "x_val, y_val = create_dataset(val_swell,look_back)\n",
    "x_test, y_test = create_dataset(test_swell, look_back)\n",
    "\n",
    "\n",
    "# 체크포인트 생성,\n",
    "checkpoint = create_checkpoint('lstm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.1 13.  13.1 13.1 13.3 13.6 13.  12.6 13.  13.6]\n"
     ]
    }
   ],
   "source": [
    "#print(x_train[0])\n",
    "print(y_train[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1032"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(target)*0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(674, 14, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리\n",
    "'''\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], x_val.shape[1], x_val.shape[2]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1],x_test.shape[2]))\n",
    "'''\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], x_val.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1],1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 10s 15ms/step - loss: 48.0905 - val_loss: 19.1074\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 19.10741, saving model to ./swellModel/lstm\\01-19.1074.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 20.4068 - val_loss: 18.5528\n",
      "\n",
      "Epoch 00001: val_loss improved from 19.10741 to 18.55282, saving model to ./swellModel/lstm\\01-18.5528.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 21.8281 - val_loss: 18.5589\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 20.8596 - val_loss: 18.8963\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 22.4239 - val_loss: 18.5903\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 21.3838 - val_loss: 18.6554\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 21.1379 - val_loss: 18.5753\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 21.3692 - val_loss: 18.5747\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 20.4041 - val_loss: 18.5962\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 19.8844 - val_loss: 19.3436\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 11ms/step - loss: 17.5535 - val_loss: 19.5762\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 22.0364 - val_loss: 18.6554\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 20.9330 - val_loss: 18.6638\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 21.9416 - val_loss: 19.1948\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 19.4055 - val_loss: 18.9626\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 20.2916 - val_loss: 18.8697\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 18.55282\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 19.7159 - val_loss: 17.3503\n",
      "\n",
      "Epoch 00001: val_loss improved from 18.55282 to 17.35034, saving model to ./swellModel/lstm\\01-17.3503.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 16.9611 - val_loss: 13.3737\n",
      "\n",
      "Epoch 00001: val_loss improved from 17.35034 to 13.37368, saving model to ./swellModel/lstm\\01-13.3737.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 16.1140 - val_loss: 12.3728\n",
      "\n",
      "Epoch 00001: val_loss improved from 13.37368 to 12.37281, saving model to ./swellModel/lstm\\01-12.3728.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 12.6135 - val_loss: 13.4744\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 12.37281\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 12.8984 - val_loss: 17.2163\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 12.37281\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 11.2794 - val_loss: 14.2249\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 12.37281\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 11ms/step - loss: 10.4199 - val_loss: 9.9419\n",
      "\n",
      "Epoch 00001: val_loss improved from 12.37281 to 9.94195, saving model to ./swellModel/lstm\\01-9.9419.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 8.7447 - val_loss: 10.2082\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 9.94195\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 8.9235 - val_loss: 11.1088\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 9.94195\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 8.0761 - val_loss: 7.6531\n",
      "\n",
      "Epoch 00001: val_loss improved from 9.94195 to 7.65314, saving model to ./swellModel/lstm\\01-7.6531.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 8.1012 - val_loss: 7.6159\n",
      "\n",
      "Epoch 00001: val_loss improved from 7.65314 to 7.61589, saving model to ./swellModel/lstm\\01-7.6159.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 7.8871 - val_loss: 8.4125\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 7.61589\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 9.0764 - val_loss: 7.3787\n",
      "\n",
      "Epoch 00001: val_loss improved from 7.61589 to 7.37866, saving model to ./swellModel/lstm\\01-7.3787.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 8.3050 - val_loss: 7.1747\n",
      "\n",
      "Epoch 00001: val_loss improved from 7.37866 to 7.17471, saving model to ./swellModel/lstm\\01-7.1747.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 7.4836 - val_loss: 6.6088\n",
      "\n",
      "Epoch 00001: val_loss improved from 7.17471 to 6.60877, saving model to ./swellModel/lstm\\01-6.6088.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 7.7904 - val_loss: 6.7951\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.60877\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 6.6412 - val_loss: 5.5847\n",
      "\n",
      "Epoch 00001: val_loss improved from 6.60877 to 5.58472, saving model to ./swellModel/lstm\\01-5.5847.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 7.2285 - val_loss: 6.5250\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 5.58472\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 6.6954 - val_loss: 6.7435\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 5.58472\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 6.5017 - val_loss: 6.5171\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 5.58472\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 6.2008 - val_loss: 5.1961\n",
      "\n",
      "Epoch 00001: val_loss improved from 5.58472 to 5.19610, saving model to ./swellModel/lstm\\01-5.1961.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674/674 [==============================] - 6s 9ms/step - loss: 6.2759 - val_loss: 5.7492\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 5.19610\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 6.0210 - val_loss: 5.4818\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 5.19610\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 6.6195 - val_loss: 5.5103\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 5.19610\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 6.2199 - val_loss: 4.7736\n",
      "\n",
      "Epoch 00001: val_loss improved from 5.19610 to 4.77358, saving model to ./swellModel/lstm\\01-4.7736.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 6.2959 - val_loss: 6.1872\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 4.77358\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 6.1971 - val_loss: 5.2461\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 4.77358\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 5.3640 - val_loss: 5.4042\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 4.77358\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 6.2607 - val_loss: 5.5363\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 4.77358\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 5.8163 - val_loss: 3.3963\n",
      "\n",
      "Epoch 00001: val_loss improved from 4.77358 to 3.39634, saving model to ./swellModel/lstm\\01-3.3963.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 5.6085 - val_loss: 4.9755\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 5.4521 - val_loss: 4.5108\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 5.2165 - val_loss: 5.8019\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 6.9248 - val_loss: 5.7484\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 6.2254 - val_loss: 4.3431\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 6.0116 - val_loss: 5.0891\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 5.9558 - val_loss: 6.2980\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 5.5144 - val_loss: 4.6241\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 5.0532 - val_loss: 4.6816\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 5.1533 - val_loss: 4.6744\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 14s 20ms/step - loss: 5.4240 - val_loss: 3.4146\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.39634\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 12ms/step - loss: 4.9262 - val_loss: 3.0278\n",
      "\n",
      "Epoch 00001: val_loss improved from 3.39634 to 3.02785, saving model to ./swellModel/lstm\\01-3.0278.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 12ms/step - loss: 4.8548 - val_loss: 6.3149\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 5.2743 - val_loss: 5.9607\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 4.2776 - val_loss: 5.6352\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 5.0805 - val_loss: 5.9965\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.5760 - val_loss: 5.2071\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.7833 - val_loss: 5.8048\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.6572 - val_loss: 3.7593\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.5455 - val_loss: 3.7305\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 4.6162 - val_loss: 3.2939\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.4802 - val_loss: 3.0966\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 4.6272 - val_loss: 3.3336\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 3.02785\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 3.8665 - val_loss: 2.2747\n",
      "\n",
      "Epoch 00001: val_loss improved from 3.02785 to 2.27465, saving model to ./swellModel/lstm\\01-2.2747.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.5068 - val_loss: 3.5567\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.4010 - val_loss: 3.3561\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.8949 - val_loss: 2.7596\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.2761 - val_loss: 3.8976\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.9020 - val_loss: 3.7800\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 3.7217 - val_loss: 3.7509\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.1128 - val_loss: 4.0615\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674/674 [==============================] - 7s 10ms/step - loss: 3.6042 - val_loss: 4.0442\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.1633 - val_loss: 4.6587\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.3955 - val_loss: 5.9839\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 4.2350 - val_loss: 4.9681\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.2504 - val_loss: 4.4211\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 3.7260 - val_loss: 5.0234\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.8913 - val_loss: 5.5214\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.6964 - val_loss: 5.1263\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.9289 - val_loss: 4.9487\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.9496 - val_loss: 5.4372\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.5426 - val_loss: 4.2325\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.8316 - val_loss: 4.9399\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.7733 - val_loss: 3.7437\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 4.0109 - val_loss: 4.2233\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 3.7723 - val_loss: 3.2023\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.6856 - val_loss: 5.5612\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.4756 - val_loss: 3.2618\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.5768 - val_loss: 3.1690\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.4363 - val_loss: 3.6472\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.9145 - val_loss: 2.9001\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.2123 - val_loss: 3.7499\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.3160 - val_loss: 4.0675\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.2676 - val_loss: 3.8832\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.1821 - val_loss: 4.0736\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.3342 - val_loss: 3.2852\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.3564 - val_loss: 3.5644\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.3046 - val_loss: 3.4510\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.1835 - val_loss: 2.7573\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.2926 - val_loss: 2.9933\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.3451 - val_loss: 3.0583\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 3.0641 - val_loss: 5.7455\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.2966 - val_loss: 4.7334\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.3940 - val_loss: 4.5567\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 3.3077 - val_loss: 4.1349\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 12ms/step - loss: 2.9089 - val_loss: 4.0368\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 17s 25ms/step - loss: 3.0441 - val_loss: 4.0455\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 9s 14ms/step - loss: 3.0870 - val_loss: 4.0640\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 9s 14ms/step - loss: 3.0055 - val_loss: 3.2007\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.7359 - val_loss: 2.9305\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.9151 - val_loss: 2.6605\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.27465\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.8156 - val_loss: 2.2536\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.27465 to 2.25363, saving model to ./swellModel/lstm\\01-2.2536.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674/674 [==============================] - 7s 11ms/step - loss: 2.5467 - val_loss: 2.3436\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.25363\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.6754 - val_loss: 2.5151\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 2.25363\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.6777 - val_loss: 2.1189\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.25363 to 2.11892, saving model to ./swellModel/lstm\\01-2.1189.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.8147 - val_loss: 1.9901\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.11892 to 1.99012, saving model to ./swellModel/lstm\\01-1.9901.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.5968 - val_loss: 1.7419\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.99012 to 1.74192, saving model to ./swellModel/lstm\\01-1.7419.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 12ms/step - loss: 2.8723 - val_loss: 1.1665\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.74192 to 1.16651, saving model to ./swellModel/lstm\\01-1.1665.hdf5\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.6376 - val_loss: 2.1345\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.7870 - val_loss: 2.0433\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.7002 - val_loss: 2.6337\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 12ms/step - loss: 2.7331 - val_loss: 2.3186\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.6616 - val_loss: 1.9445\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.7696 - val_loss: 2.5229\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.5811 - val_loss: 2.5987\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.6102 - val_loss: 2.3402\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 12ms/step - loss: 2.5055 - val_loss: 2.3723\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.4358 - val_loss: 2.0939\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.4859 - val_loss: 1.9913\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.4870 - val_loss: 2.3937\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.4581 - val_loss: 2.4866\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.1567 - val_loss: 2.5871\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.4169 - val_loss: 4.2844\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 14s 21ms/step - loss: 2.3227 - val_loss: 3.6374\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 11s 16ms/step - loss: 2.3763 - val_loss: 2.9456\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 9s 14ms/step - loss: 2.1651 - val_loss: 2.4423\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 11ms/step - loss: 2.4776 - val_loss: 3.7978\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.2562 - val_loss: 1.9072\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 11ms/step - loss: 1.9156 - val_loss: 2.3724\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.3835 - val_loss: 2.5476\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.1774 - val_loss: 3.2121\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.1810 - val_loss: 3.0487\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.3345 - val_loss: 2.7072\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 10s 14ms/step - loss: 2.1471 - val_loss: 2.5363\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 12ms/step - loss: 2.1286 - val_loss: 2.4393\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 11ms/step - loss: 2.0545 - val_loss: 2.7972\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.0480 - val_loss: 2.7324\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.2358 - val_loss: 3.3350\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.4582 - val_loss: 2.9910\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.2604 - val_loss: 3.1620\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.7832 - val_loss: 2.9149\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 12s 18ms/step - loss: 2.1287 - val_loss: 2.9847\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674/674 [==============================] - 11s 17ms/step - loss: 2.2937 - val_loss: 2.7365\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 9s 13ms/step - loss: 2.2639 - val_loss: 3.3090\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 11ms/step - loss: 2.2559 - val_loss: 3.4879\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.2356 - val_loss: 3.2635\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.0674 - val_loss: 3.9445\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.1038 - val_loss: 3.1949\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.2875 - val_loss: 3.0853\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 9s 13ms/step - loss: 2.0150 - val_loss: 2.4408\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 9s 14ms/step - loss: 1.9884 - val_loss: 2.9259\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 2.1130 - val_loss: 3.0968\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.8684 - val_loss: 4.2864\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.4929 - val_loss: 3.7479\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.1824 - val_loss: 2.7520\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.9976 - val_loss: 2.3366\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.9982 - val_loss: 2.1494\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.1936 - val_loss: 1.9270\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 9s 13ms/step - loss: 2.1798 - val_loss: 2.2472\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.1275 - val_loss: 2.3753\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.8602 - val_loss: 2.4504\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.7121 - val_loss: 2.8289\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.8840 - val_loss: 2.0515\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.8993 - val_loss: 2.7449\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.7951 - val_loss: 2.6168\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 1.9198 - val_loss: 2.8663\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 11s 16ms/step - loss: 1.7847 - val_loss: 2.6198\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 9s 14ms/step - loss: 1.7839 - val_loss: 3.1079\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 8s 11ms/step - loss: 2.0641 - val_loss: 2.3266\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.8541 - val_loss: 2.8523\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.8684 - val_loss: 2.1692\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.8150 - val_loss: 3.6092\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.6196 - val_loss: 2.5687\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.7202 - val_loss: 2.8970\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.9077 - val_loss: 3.2115\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 2.0336 - val_loss: 3.0433\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.7441 - val_loss: 3.1523\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.6531 - val_loss: 3.4251\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.6559 - val_loss: 3.6751\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.7608 - val_loss: 3.1453\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.6354 - val_loss: 3.8190\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.7515 - val_loss: 2.9083\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.8337 - val_loss: 3.4510\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.6688 - val_loss: 3.0135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.8122 - val_loss: 2.9101\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.8379 - val_loss: 2.5019\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.4826 - val_loss: 2.5368\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.6565 - val_loss: 2.6717\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.5967 - val_loss: 2.3012\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.6555 - val_loss: 2.6599\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 1.5244 - val_loss: 2.8904\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 9ms/step - loss: 1.7446 - val_loss: 2.5747\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 6s 10ms/step - loss: 1.6232 - val_loss: 2.6228\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.5290 - val_loss: 2.6791\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 1.7218 - val_loss: 3.0142\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 1.5061 - val_loss: 2.5179\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.7661 - val_loss: 2.5425\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 10ms/step - loss: 1.6140 - val_loss: 2.3041\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 1.5372 - val_loss: 2.4633\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 7s 11ms/step - loss: 1.6362 - val_loss: 2.3058\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.16651\n",
      "Train on 674 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "130/674 [====>.........................] - ETA: 5s - loss: 0.5163"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-54d3307aaced>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcustom_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.2.0-py3.6.egg\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.2.0-py3.6.egg\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.2.0-py3.6.egg\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.2.0-py3.6.egg\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2. 모델 구성하기\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(32, batch_input_shape=(1, look_back, feature_num), stateful=True, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(CuDNNLSTM(32, batch_input_shape=(1, look_back, feature_num), stateful=True, return_sequences=True))\n",
    "model.add(Dropout(0.3))      \n",
    "model.add(CuDNNLSTM(32, batch_input_shape=(1, look_back, feature_num), stateful=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(32, batch_input_shape=(1, look_back, 1), stateful=True, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(CuDNNLSTM(32, batch_input_shape=(1, look_back, 1), stateful=True, return_sequences=True))\n",
    "model.add(Dropout(0.3))      \n",
    "model.add(CuDNNLSTM(32, batch_input_shape=(1, look_back, 1), stateful=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#model.load_weights('./swellModel/lstm/271-1.8620.hdf5')\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "custom_hist = CustomHistory()\n",
    "custom_hist.init()\n",
    "\n",
    "for i in range(500):\n",
    "    model.fit(x_train, y_train, epochs=1, batch_size=1, shuffle=False, callbacks=[custom_hist, checkpoint], validation_data=(x_val, y_val))\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 학습과정 살펴보기\n",
    "plt.plot(custom_hist.train_loss)\n",
    "plt.plot(custom_hist.val_loss)\n",
    "plt.ylim(0.0, 40)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 모델 평가하기\n",
    "trainScore = model.evaluate(x_train, y_train, batch_size=1, verbose=0)\n",
    "model.reset_states()\n",
    "print('Train Score: ', trainScore)\n",
    "valScore = model.evaluate(x_val, y_val, batch_size=1, verbose=0)\n",
    "model.reset_states()\n",
    "print('Validataion Score: ', valScore)\n",
    "testScore = model.evaluate(x_test, y_test, batch_size=1, verbose=0)\n",
    "model.reset_states()\n",
    "print('Test Score: ', testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 모델 사용하기\n",
    "look_ahead = len(x_test)\n",
    "xhat = x_test[1]\n",
    "predictions = np.zeros((look_ahead,1))\n",
    "for i in range(look_ahead):\n",
    "    prediction = model.predict(np.array([xhat]), batch_size=1)\n",
    "    predictions[i] = prediction\n",
    "    xhat = np.vstack([xhat[1:],prediction])\n",
    " \n",
    "   \n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(np.arange(look_ahead),predictions,'r',label=\"prediction\")\n",
    "plt.plot(np.arange(look_ahead),y_test[:look_ahead],label=\"test function\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(empty_y.shape)\n",
    "print(empty_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Guryongpo_DF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Wolopo_DF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sumColumnData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2015-9-25 ~\n",
    "#Column_Sum_DF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumRowData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2014-01-01 ~\n",
    "#Row_Sum_DF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = np.cos(np.arange(1600)*(20*np.pi/1000))[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(signal_data, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(signal_data)-look_back):\n",
    "        dataX.append(signal_data[i:(i+look_back), 0])\n",
    "        dataY.append(signal_data[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
